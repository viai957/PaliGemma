{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SiglipvisionConfig:\n",
    "\n",
    "    def __init__(\n",
    "      self,\n",
    "      hidden_size = 768, # dimention of the mdoel\n",
    "      intermediate_size=3072, # linear layer\n",
    "      num_hidden_layer = 12, # no_hidden_layers\n",
    "      num_attention_heads = 12,\n",
    "      num_channels=3,\n",
    "      image_size=224,\n",
    "      patch_size=16,\n",
    "      layer_norm_eps=1e-6,\n",
    "      attention_dropout=0.0,\n",
    "      num_image_tokens: int = None,\n",
    "      **kwargs      \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layer\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_channels = num_channels\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.num_image_tokens = num_image_tokens\n",
    "\n",
    "class SiglipVisionEmbedding(nn.Module):\n",
    "    def __init__(self, config= SiglipvisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.image_size = config.imgage_size\n",
    "        self.patch_size = config.patch_size\n",
    "\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels = self.embed_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=\"valid\", # This indicates no padding is added\n",
    "        )\n",
    "        \n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        self.num_positions = self.num_patches\n",
    "        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.num_positions).expand((1, -1)),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
    "        _, _, height, width = pixel_values.shape # [Batch_Size, Channels, Height, Width]\n",
    "        # Convolve the 'patch_size' kernal over the image, with no overlapping patches since the stide is equal\n",
    "        # The output of the convolution will have shape [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W]\n",
    "        # where Num_Patches_H = height // patch_size and Num_Patches_W = width //  patch_size\n",
    "        patch_embeds = self.patch_embedding(pixel_values)\n",
    "        # [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] -> [Batch_Size, Embed_dim, Num_Patches]\n",
    "        # where Num_Patches = Num_Patches_H * Num_Patches_W\n",
    "        embeddings = patch_embeds.flatten(2)\n",
    "        # [Batch_Size, Embed_Dim, Num_Patches] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        embeddings = embeddings.transpose(1,2)\n",
    "        # Add positional encodings to each patch. Each Positional encoding is a vector of size\n",
    "        embeddings = embeddings + self.position_embedding(self.position_embedding)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        return embeddings\n",
    "    \n",
    "class SiglipAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention from 'Attention is All You Need' paper \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5 # Equivalent to 1 / sqrt(self.head_dim)\n",
    "        self.dropout = config.attention_dropout\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias = False)\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        # hidden_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        # query-states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        # key_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        # value_states: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "        # query_states: [Batch_Size, Num_Patches, Num_Heads, Head_Dim]\n",
    "        # in transpose [Batch_Size, Num_Patches, Num_Heads, Head_Dim] -> [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Calculate the attention using the formula Q * K^T / sqrt(d_k) . attn_weights : [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        attention_weights = (query_states @ key_states.transpose(2,3) * self.scale)\n",
    "\n",
    "        if attention_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
    "                f\"{attention_weights.size()}\"\n",
    "            )\n",
    "        \n",
    "        # Apply the softmax row-wise atten_weights: [Batch_Size, Num_Heads, Num_Patches, Num_Patches]\n",
    "        attention_weights = nn.function.softmax(attention_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        # Apply dropout only during training\n",
    "        attention_weights = nn.functional.dropout(attention_weights, p=self.dropout, training=self.training)\n",
    "        # Multiply the attention weights by the value_states. atten_output: [Batch_Size, Num_Heads, Num_Patches, Head_Dim]\n",
    "        attention_output = (attention_weights @ value_states)\n",
    "\n",
    "        if attention_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
    "                f\" {attention_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        # [Batch_Size, Num_Heads, Num_Patches, Head_Dim] -> [Batch_Size, Num_Patches, Num_Heads, Head_Dim]\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous()\n",
    "        # [Batch_Size, Num_Patches, Num_Heads, Head_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        attention_output = attention_output.reshape(batch_size, seq_len, self.embed_dim)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        attention_output = self.out_proj(attention_output)\n",
    "        return attention_output, attention_weights\n",
    "\n",
    "        \n",
    "class SiglipMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config \n",
    "        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Intermediate_Size]\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        # hidden_states : [Batch_Size, Num_Patches, Intermediate_Size]\n",
    "        hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
    "        # [Batch_Size, Num_Patches, Intermediate_Size] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class SiglipEncoderLayer(nn.Module):\n",
    "    def __init__(self, config : SiglipvisionConfig):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.self_atten = SiglipAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "        self.mlp = SiglipMLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states : torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # residual : [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        residual = hidden_states\n",
    "        # [Batch_Size, Num_patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states, _  = self.self_atten(hidden_states=hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = residual + hidden_states\n",
    "        # residual : [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        residual = hidden_states\n",
    "        # [Batch_Size, Number_Patchesm Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        # [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "class SiglipEncoder(nn.Module):\n",
    "    def _init__(self, config: SiglipvisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList(\n",
    "            [SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            inputs_embeds: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # inputs_embeds: [Batch_Size, Num_Patches, Embed_Dim]\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        for encoder_layer in self.layer:\n",
    "            # [Batch_Size, Num_Patches, Embed_Dim] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "            hidden_states = encoder_layer(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "    \n",
    "    \n",
    "\n",
    "class SiglipVisionTransformer(nn.Module):\n",
    "    def __init__(self, config: SiglipvisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.embeddings = SiglipVisionEmbedding(config)\n",
    "        self.encoder = SiglipEncoder(config)\n",
    "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
    "        # pixel_values: [Batch_Size, Channels, Height, Width] -> [Batch_Size, Num_Patches, Embedding_Dimention]\n",
    "        hidden_states = self.embeddings(pixel_values)\n",
    "\n",
    "        last_hidden_state = self.encoder(input_embeds=hidden_states)\n",
    "\n",
    "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
    "\n",
    "        return last_hidden_state\n",
    "\n",
    "class SiglipVisionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: SiglipvisionConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_model = SiglipVisionTransformer(config)\n",
    "\n",
    "    def forward(self, pixel_values) -> Tuple:\n",
    "        # [Batch_Size, Channels, Height, Width] -> [Batch_size, Num_Patches, Embed_Dim]\n",
    "        return self.vision_model(pixel_values=pixel_values)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
